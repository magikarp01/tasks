{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the various tasks in the `tasks` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/phillip_guo/facts-sae'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# os.chdir('../mechanistic-unlearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# from cb_utils.models import load_gpt2_weights, load_demo_gpt2, tokenizer as gpt2_tokenizer, DEVICE\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from tasks.inference_utils import get_final_logits, generate_text\n",
    "from tasks.ioi.IOITask import IOITask_old, IOITask\n",
    "from tasks.owt.OWTTask import OWTTask\n",
    "from tasks.facts.SportsTask import SportsTask\n",
    "from transformer_lens import HookedTransformer\n",
    "# from tasks.kg_trips.ZSRETask import ZSRE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New IOI Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# model = load_demo_gpt2(means=False)\n",
    "model = HookedTransformer.from_pretrained('gpt2-small')\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec42afbe5d74329b362d3eb4d14fc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151754b12beb4731863ab0e660e981c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tasks.general.DatasetTasks import OWTTask, PileTask\n",
    "owt = OWTTask(64, tokenizer)\n",
    "pile = PileTask(64, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6148, device='cuda:0')\n",
      "tensor(3.7921, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(owt.get_test_loss(model))\n",
    "print(pile.get_test_loss(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Then, Crystal and Jeffrey went to the hospital. Jeffrey gave a snack to', 'Then, Patrick and Matthew went to the office. Matthew gave a necklace to', 'Then, Adam and Brian went to the store. Brian gave a basketball to', 'Then, Samantha and Matthew went to the garden. Matthew gave a kiss to', 'Then, Andrew and Justin went to the office. Justin gave a drink to', 'Then, Jacob and Sara went to the station. Sara gave a necklace to', 'Then, Samuel and Joshua went to the house. Joshua gave a necklace to', 'Then, Gregory and Megan went to the house. Megan gave a necklace to', 'Then, Allison and Benjamin went to the station. Benjamin gave a drink to', 'Then, Jose and Heather went to the school. Heather gave a snack to'], 'IO': ['Crystal', 'Patrick', 'Adam', 'Samantha', 'Andrew', 'Jacob', 'Samuel', 'Gregory', 'Allison', 'Jose'], 'S': ['Jeffrey', 'Matthew', 'Brian', 'Matthew', 'Justin', 'Sara', 'Joshua', 'Megan', 'Benjamin', 'Heather']}\n"
     ]
    }
   ],
   "source": [
    "old_ioi = IOITask_old(10, gpt2_tokenizer)\n",
    "print(next(old_ioi.train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.ioi.IOITask import IOIData\n",
    "N = 1000\n",
    "clean_dataset = IOIData(\n",
    "    prompt_type='ABBA',\n",
    "    N=N,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    nb_templates=1,\n",
    "    device=DEVICE\n",
    ")\n",
    "corr_dataset = clean_dataset.gen_flipped_prompts('ABC->XYZ, BAB->XYZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PLACE]': ['garden', 'house', 'house', 'store', 'restaurant', 'school', 'garden', 'school', 'office', 'hospital', 'hospital', 'house', 'restaurant', 'station', 'restaurant', 'school', 'office', 'hospital', 'school', 'school', 'school', 'station', 'restaurant', 'store', 'hospital', 'garden', 'hospital', 'restaurant', 'school', 'house', 'station', 'school'], '[OBJECT]': ['computer', 'ring', 'snack', 'drink', 'drink', 'necklace', 'bone', 'basketball', 'kiss', 'drink', 'ring', 'necklace', 'necklace', 'basketball', 'drink', 'kiss', 'kiss', 'ring', 'basketball', 'computer', 'ring', 'basketball', 'computer', 'basketball', 'snack', 'basketball', 'bone', 'computer', 'computer', 'snack', 'snack', 'drink'], 'text': ['Then, George and Tyler went to the garden. Tyler gave a computer to George', 'Then, Ruby and Laura went to the house. Laura gave a ring to Ruby', 'Then, Alan and Dean went to the house. Dean gave a snack to Alan', 'Then, Crew and Kate went to the store. Kate gave a drink to Crew', 'Then, Georgia and Andy went to the restaurant. Andy gave a drink to Georgia', 'Then, Jane and Amy went to the school. Amy gave a necklace to Jane', 'Then, Mark and Kelly went to the garden. Kelly gave a bone to Mark', 'Then, Roman and Laura went to the school. Laura gave a basketball to Roman', 'Then, Kate and Brandon went to the office. Brandon gave a kiss to Kate', 'Then, George and Russell went to the hospital. Russell gave a drink to George', 'Then, Christian and Warren went to the hospital. Warren gave a ring to Christian', 'Then, Laura and Mark went to the house. Mark gave a necklace to Laura', 'Then, Alex and Morgan went to the restaurant. Morgan gave a necklace to Alex', 'Then, Joseph and Sarah went to the station. Sarah gave a basketball to Joseph', 'Then, Paul and Laura went to the restaurant. Laura gave a drink to Paul', 'Then, Anderson and John went to the school. John gave a kiss to Anderson', 'Then, Max and Kevin went to the office. Kevin gave a kiss to Max', 'Then, Madison and Roman went to the hospital. Roman gave a ring to Madison', 'Then, Sullivan and Justin went to the school. Justin gave a basketball to Sullivan', 'Then, William and Joseph went to the school. Joseph gave a computer to William', 'Then, Marco and Jason went to the school. Jason gave a ring to Marco', 'Then, Jacob and David went to the station. David gave a basketball to Jacob', 'Then, Charlie and Crew went to the restaurant. Crew gave a computer to Charlie', 'Then, Jennifer and David went to the store. David gave a basketball to Jennifer', 'Then, William and Aaron went to the hospital. Aaron gave a snack to William', 'Then, Simon and Brandon went to the garden. Brandon gave a basketball to Simon', 'Then, Austin and Robert went to the hospital. Robert gave a bone to Austin', 'Then, Adam and Kelly went to the restaurant. Kelly gave a computer to Adam', 'Then, Jordan and Henry went to the school. Henry gave a computer to Jordan', 'Then, Anthony and Roman went to the house. Roman gave a snack to Anthony', 'Then, Martin and Rachel went to the station. Rachel gave a snack to Martin', 'Then, Michelle and Mark went to the school. Mark gave a drink to Michelle'], 'IO': ['George', 'Ruby', 'Alan', 'Crew', 'Georgia', 'Jane', 'Mark', 'Roman', 'Kate', 'George', 'Christian', 'Laura', 'Alex', 'Joseph', 'Paul', 'Anderson', 'Max', 'Madison', 'Sullivan', 'William', 'Marco', 'Jacob', 'Charlie', 'Jennifer', 'William', 'Simon', 'Austin', 'Adam', 'Jordan', 'Anthony', 'Martin', 'Michelle'], 'S': ['Tyler', 'Laura', 'Dean', 'Kate', 'Andy', 'Amy', 'Kelly', 'Laura', 'Brandon', 'Russell', 'Warren', 'Mark', 'Morgan', 'Sarah', 'Laura', 'John', 'Kevin', 'Roman', 'Justin', 'Joseph', 'Jason', 'David', 'Crew', 'David', 'Aaron', 'Brandon', 'Robert', 'Kelly', 'Henry', 'Roman', 'Rachel', 'Mark'], 'TEMPLATE_IDX': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "# make a dataset that is compatible with dataloader of clean_dataset.ioi_prompts\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IOIPromptsDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item\n",
    "\n",
    "# Create dataset instance\n",
    "ioi_prompts_dataset = IOIPromptsDataset(clean_dataset.ioi_prompts)\n",
    "\n",
    "# Create DataLoader instance\n",
    "dataloader = DataLoader(ioi_prompts_dataset, batch_size=32, shuffle=True)\n",
    "print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Then, Louis and Ruby went to the garden. Ruby gave a computer to', 'Then, Andrew and Mary went to the office. Mary gave a drink to', 'Then, Jane and Andre went to the hospital. Andre gave a kiss to', 'Then, Eric and Blake went to the restaurant. Blake gave a necklace to', 'Then, Cole and Rose went to the restaurant. Rose gave a kiss to', 'Then, Richard and Kate went to the restaurant. Kate gave a bone to', 'Then, Jamie and John went to the hospital. John gave a basketball to', 'Then, Edward and Georgia went to the hospital. Georgia gave a computer to', 'Then, Sullivan and Steven went to the restaurant. Steven gave a computer to', 'Then, David and Dean went to the hospital. Dean gave a snack to']\n"
     ]
    }
   ],
   "source": [
    "new_ioi = IOITask(10, gpt2_tokenizer, prep_acdcpp=True)\n",
    "print(next(new_ioi.train_iter)['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_demo_gpt2(means=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(d_model=768, debug=False, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_old = IOITask_old(batch_size=100, tokenizer=tokenizer)\n",
    "ioi_new = IOITask(batch_size=100, tokenizer=tokenizer)\n",
    "owt = OWTTask(batch_size=3, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tasks/ioi/data/ioi_prompts_single_template_train.pkl\", \"rb\") as f:\n",
    "    ioi_prompts_train = pickle.load(f)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IOIPromptsDataset(Dataset):\n",
    "    def __init__(self, ioi_prompts, tokenizer):\n",
    "        self.ioi_prompts = ioi_prompts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ioi_prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.ioi_prompts[idx]\n",
    "        text = prompt['text']\n",
    "        text = \" \".join(text.split(\" \")[:-1])\n",
    "        label = prompt['IO']\n",
    "        return {\n",
    "            'text': text,\n",
    "            'IO': label,\n",
    "            'S': prompt['S']\n",
    "        }\n",
    "ioi_dataset = IOIPromptsDataset(ioi_prompts_train, tokenizer)\n",
    "def collate_batch(batch):\n",
    "\n",
    "    texts = [item['text'] for item in batch]\n",
    "    ios = [item['IO'] for item in batch]\n",
    "    subjects = [item['S'] for item in batch]\n",
    "    return {\n",
    "        'text': texts,\n",
    "        'IO': ios,\n",
    "        'S': subjects\n",
    "    }\n",
    "\n",
    "ioi_dataloader = DataLoader(ioi_dataset, batch_size=100, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ioi_texts = [ioi.ioi_prompts_train_dataset[i]['text'] for i in range(3)]\n",
    "# for i in range(3):\n",
    "#     ioi_texts.append(ioi_texts[i][5:])=\n",
    "ioi_texts = ['Then, Sarah and Tyler went to the garden. Tyler gave a bone to Sarah',\n",
    " 'Then, Tyler and Sarah went to the garden. Sarah gave a bone to Tyler',\n",
    " 'Then, Timothy and Stephen went to the school. Stephen gave a necklace to Timothy',\n",
    " 'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to Sarah',\n",
    " 'Then, Tyler and Sarah went to the flower garden. Sarah gave a bone to Tyler',\n",
    " 'Then, Timothy and Stephen went to the old school. Stephen gave a necklace to Timothy']\n",
    "\n",
    "# cut last name from ioi_texts\n",
    "for i in range(6):\n",
    "    ioi_texts[i] = ioi_texts[i][:-len(ioi_texts[i].split()[-1])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then:,: Sarah: and: Tyler: went: to: the: garden:.: Tyler: gave: a: bone: to:<|endoftext|>:\n",
      "Then:,: Tyler: and: Sarah: went: to: the: garden:.: Sarah: gave: a: bone: to:<|endoftext|>:\n",
      "Then:,: Timothy: and: Stephen: went: to: the: school:.: Stephen: gave: a: necklace: to:<|endoftext|>:\n",
      "Then:,: Sarah: and: Tyler: went: to: the: flower: garden:.: Tyler: gave: a: bone: to:\n",
      "Then:,: Tyler: and: Sarah: went: to: the: flower: garden:.: Sarah: gave: a: bone: to:\n",
      "Then:,: Timothy: and: Stephen: went: to: the: old: school:.: Stephen: gave: a: necklace: to:\n"
     ]
    }
   ],
   "source": [
    "# tokenize ioi_texts\n",
    "tokens = tokenizer(ioi_texts, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# detokenize ioi_texts, token by token\n",
    "for i in range(6):\n",
    "    for token in tokens[i]:\n",
    "        print(tokenizer.decode(token.item()), end=':')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 15, 15, 16, 16, 16]\n",
      " Sarah Tyler Timothy Sarah Tyler Timothy"
     ]
    }
   ],
   "source": [
    "final_logits = get_final_logits(model, tokenizer, ioi_texts)\n",
    "\n",
    "# decode final_logits\n",
    "for i in range(6):\n",
    "    print(tokenizer.decode(final_logits[i].argmax(-1).tolist()), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 84.986691584, Reserved: 1.9566428160000002, Allocated: 1.1659540480000001, Free: 0.7906887680000001\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"Total: {t*1e-9}, Reserved: {r*1e-9}, Allocated: {a*1e-9}, Free: {f*1e-9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to Sarah, and'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, 'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.kg_trips.ZSRETask import MENDQADataset\n",
    "mendqa_dataset = MENDQADataset(data_dir=\"tasks/kg_trips\", tok=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'case_id': 0, 'requested_rewrite': {'prompt': 'What university did {} attend?', 'subject': 'Watts Humphrey', 'target_new': {'str': 'Illinois Institute of Technology'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': ['What university did Watts Humphrey take part in?'], 'neighborhood_prompts': [{'prompt': 'nq question: who played desmond doss father in hacksaw ridge?', 'target': ' Hugo'}, {'prompt': 'nq question: who played desmond doss father in hacksaw ridge? Hugo', 'target': ' We'}, {'prompt': 'nq question: who played desmond doss father in hacksaw ridge? Hugo We', 'target': 'aving'}], 'attribute_prompts': [], 'generation_prompts': []}\n",
      "{'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does {} belong to?', 'subject': 'Ramalinaceae', 'target_new': {'str': 'Lecanorales'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': ['What family are Ramalinaceae?'], 'neighborhood_prompts': [{'prompt': 'nq question: types of skiing in the winter olympics 2018?', 'target': ' Down'}, {'prompt': 'nq question: types of skiing in the winter olympics 2018? Down', 'target': 'hill'}], 'attribute_prompts': [], 'generation_prompts': []}\n",
      "{'case_id': 2, 'requested_rewrite': {'prompt': 'What role does {} play in football?', 'subject': 'Denny Herzig', 'target_new': {'str': 'defender'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': [\"What's Denny Herzig's role in football?\"], 'neighborhood_prompts': [{'prompt': 'nq question: where does aarp fall on the political spectrum?', 'target': ' non'}, {'prompt': 'nq question: where does aarp fall on the political spectrum? non', 'target': '-'}, {'prompt': 'nq question: where does aarp fall on the political spectrum? non-', 'target': 'partisan'}], 'attribute_prompts': [], 'generation_prompts': []}\n"
     ]
    }
   ],
   "source": [
    "# call the __getitem__ method of the dataset\n",
    "for i in range(3):\n",
    "    print(mendqa_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythia Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "pythia_model = HookedTransformer.from_pretrained(\n",
    "    \"pythia-70m\"\n",
    ")\n",
    "pythia_tokenizer = pythia_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>athlete</th>\n",
       "      <th>sport</th>\n",
       "      <th>log_prob_one_shot</th>\n",
       "      <th>num_athlete_tokens</th>\n",
       "      <th>sport_index</th>\n",
       "      <th>sport_token</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642</td>\n",
       "      <td>DeForest Buckner</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.492917</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>738</td>\n",
       "      <td>Walter Payton</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.105714</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16778</td>\n",
       "      <td>Anthony DeSclafani</td>\n",
       "      <td>baseball</td>\n",
       "      <td>-0.292668</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14623</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14501</td>\n",
       "      <td>Kevin Millwood</td>\n",
       "      <td>baseball</td>\n",
       "      <td>-0.372979</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14623</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>Vonta Leach</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.648644</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             athlete     sport  log_prob_one_shot  \\\n",
       "0        1642    DeForest Buckner  football          -0.492917   \n",
       "1         738       Walter Payton  football          -0.105714   \n",
       "2       16778  Anthony DeSclafani  baseball          -0.292668   \n",
       "3       14501      Kevin Millwood  baseball          -0.372979   \n",
       "4         188         Vonta Leach  football          -0.648644   \n",
       "\n",
       "   num_athlete_tokens  sport_index  sport_token  \\\n",
       "0                   5            2         5842   \n",
       "1                   3            2         5842   \n",
       "2                   6            0        14623   \n",
       "3                   3            0        14623   \n",
       "4                   5            2         5842   \n",
       "\n",
       "                                              prompt  \n",
       "0  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "1  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "2  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "3  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "4  Fact: Tiger Woods plays the sport of golf\\nFac...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tasks/facts/data/sports.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: DeForest Buckner plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1762a076784ea89a48f04bf3ad6ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: DeForest Buckner plays the sport of basketball\n",
      "Correct sport: football\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Walter Payton plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0921c4d82b4130bf9daa280c0d7568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Walter Payton plays the sport of football\n",
      "Correct sport: football\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Anthony DeSclafani plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947db269fd124a92aa2b0e5b467adf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Anthony DeSclafani plays the sport of baseball\n",
      "Correct sport: baseball\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Kevin Millwood plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82abd1a2d4e04131a33a43630852e3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Kevin Millwood plays the sport of baseball\n",
      "Correct sport: baseball\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Vonta Leach plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa7611d5f034ccb9b5b0f3eeffbd067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Vonta Leach plays the sport of football\n",
      "Correct sport: football\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tasks.inference_utils import generate_text\n",
    "for i in range(5):\n",
    "    prompt = df['prompt'].iloc[i]\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(generate_text(pythia_model, pythia_tokenizer, prompt, 1))\n",
    "    print(f\"Correct sport: {df['sport'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "football_token=5842 baseball_token=14623 basketball_token=14648\n"
     ]
    }
   ],
   "source": [
    "football_token, baseball_token, basketball_token = pythia_tokenizer(\" football baseball basketball\").input_ids\n",
    "print(f\"{football_token=} {baseball_token=} {basketball_token=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.3688, -3.1681,  4.5482,  ..., -2.6834, -2.6928, -2.4418],\n",
      "        [10.1932, -2.6702,  6.6471,  ..., -2.3068, -2.4198, -2.0895],\n",
      "        [ 8.2223, -2.5343,  3.4966,  ..., -2.3035, -2.3677, -2.1004]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([ 5842,  5842, 14623])\n",
      "tensor(0.7214, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7030, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set up dataloader to batch through df\n",
    "from torch.utils.data import DataLoader\n",
    "from tasks.inference_utils import get_final_logits\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class SportsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df['prompt'].iloc[idx], self.df['sport'].iloc[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "sports_dataset = SportsDataset(df, pythia_tokenizer)\n",
    "sports_dataloader = DataLoader(sports_dataset, batch_size=3)\n",
    "\n",
    "# batch through dataloader\n",
    "for batch in sports_dataloader:\n",
    "    prompts, labels = batch\n",
    "    labels = [' ' + sport for sport in labels]\n",
    "    final_logits = get_final_logits(pythia_model, pythia_tokenizer, prompts, model_returns_tuple=False)\n",
    "    print(final_logits)\n",
    "    tokenized_labels = pythia_tokenizer(labels, return_tensors='pt', padding=True, truncation=True).input_ids[:, 0]\n",
    "    print(tokenized_labels)\n",
    "    print(criterion(final_logits, tokenized_labels))\n",
    "\n",
    "    print(criterion(final_logits, torch.Tensor([14648, 14648, 14648]).long()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1563)\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "from tasks.facts.SportsTask import SportsTask\n",
    "\n",
    "sports_task = SportsTask(batch_size=1000, tokenizer=pythia_tokenizer)\n",
    "print(sports_task.get_test_loss(pythia_model))\n",
    "print(sports_task.get_test_accuracy(pythia_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 84.986691584, Reserved: 17.471373312, Allocated: 15.943849984000002, Free: 1.527523328\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"Total: {t*1e-9}, Reserved: {r*1e-9}, Allocated: {a*1e-9}, Free: {f*1e-9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformer_lens import HookedTransformer\n",
    "pythia_model = HookedTransformer.from_pretrained(\n",
    "    \"pythia-70m\"\n",
    ")\n",
    "pythia_tokenizer = pythia_model.tokenizer\n",
    "\n",
    "ioi_pythia_old = IOITask_old(batch_size=100, tokenizer=pythia_tokenizer, handle_multitoken_labels=True)\n",
    "ioi_pythia = IOITask(batch_size=100, tokenizer=pythia_tokenizer, handle_multitoken_labels=True)\n",
    "sports_pythia = SportsTask(batch_size=100, tokenizer=pythia_tokenizer)\n",
    "owt_pythia = OWTTask(batch_size=100, tokenizer=pythia_tokenizer)\n",
    "\n",
    "ioi_gpt2_old = IOITask_old(batch_size=100, tokenizer=gpt2_tokenizer)\n",
    "ioi_gpt2 = IOITask(batch_size=100, tokenizer=gpt2_tokenizer)\n",
    "sports_gpt2 = SportsTask(batch_size=100, tokenizer=gpt2_tokenizer)\n",
    "owt_gpt2 = OWTTask(batch_size=100, tokenizer=gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Loaded pretrained model pythia-410m into HookedTransformer\n",
      "Loaded pretrained model pythia-1b into HookedTransformer\n",
      "Loaded pretrained model pythia-1.4b into HookedTransformer\n",
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "Loaded pretrained model gpt2-large into HookedTransformer\n",
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pythia model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B\n",
    "models = {\n",
    "    \"pythia-70m\": HookedTransformer.from_pretrained(\"pythia-70m\"),\n",
    "    \"pythia-160m\": HookedTransformer.from_pretrained(\"pythia-160m\"),\n",
    "    \"pythia-410m\": HookedTransformer.from_pretrained(\"pythia-410m\"),\n",
    "    \"pythia-1b\": HookedTransformer.from_pretrained(\"pythia-1b\"),\n",
    "    \"pythia-1.4b\": HookedTransformer.from_pretrained(\"pythia-1.4b\"),\n",
    "    \"pythia-2.8b\": HookedTransformer.from_pretrained(\"pythia-2.8b\"),\n",
    "\n",
    "    \"gpt2-small\": HookedTransformer.from_pretrained(\"gpt2\"),\n",
    "    \"gpt2-medium\": HookedTransformer.from_pretrained(\"gpt2-medium\"),\n",
    "    \"gpt2-large\": HookedTransformer.from_pretrained(\"gpt2-large\"),\n",
    "    \"gpt2-xl\": HookedTransformer.from_pretrained(\"gpt2-xl\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 84.986691584, Reserved: 38.247858176, Allocated: 38.035869184, Free: 0.21198899200000001\n"
     ]
    }
   ],
   "source": [
    "# how much cuda memory taken\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"Total: {t*1e-9}, Reserved: {r*1e-9}, Allocated: {a*1e-9}, Free: {f*1e-9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_225168/1757029106.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for model_name, model in tqdm(models.items()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1889618391014eff99336508c9b4cffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame(columns=['IOI-old Loss', 'IOI-old Accuracy', 'IOI Loss', 'IOI Accuracy', 'Sports Loss', 'Sports Accuracy', 'OWT Loss'])\n",
    "\n",
    "for model_name, model in tqdm(models.items()):\n",
    "    if \"pythia\" in model_name:\n",
    "        # print(f\"For {model_name} on IOI, loss is {ioi_pythia.get_test_loss(model)}, ioi accuracy is {ioi_pythia.get_test_accuracy(model)}, overall accuracy is {ioi_pythia.get_test_accuracy(model, check_all_logits=True)}\")\n",
    "        # print(f\"For {model_name} on Sports, loss is {sports_pythia.get_test_loss(model)}, sports accuracy is {sports_pythia.get_test_accuracy(model)}, overall accuracy is {sports_pythia.get_test_accuracy(model, check_all_logits=True)}\")\n",
    "\n",
    "        accuracy_df.loc[model_name] = [ioi_pythia_old.get_test_loss(model).item(), ioi_pythia_old.get_test_accuracy(model), ioi_pythia.get_test_loss(model).item(), ioi_pythia.get_test_accuracy(model), sports_pythia.get_test_loss(model).item(), sports_pythia.get_test_accuracy(model), owt_pythia.get_test_loss(model).item()]\n",
    "    \n",
    "    elif \"gpt2\" in model_name:\n",
    "        # print(f\"For {model_name} on IOI, loss is {ioi_gpt2.get_test_loss(model)}, ioi accuracy is {ioi_gpt2.get_test_accuracy(model)}, overall accuracy is {ioi_gpt2.get_test_accuracy(model, check_all_logits=True)}\")\n",
    "        # print(f\"For {model_name} on Sports, loss is {sports_gpt2.get_test_loss(model)}, sports accuracy is {sports_gpt2.get_test_accuracy(model)}, overall accuracy is {sports_gpt2.get_test_accuracy(model, check_all_logits=True)}\")\n",
    "\n",
    "        accuracy_df.loc[model_name] = [ioi_gpt2_old.get_test_loss(model).item(), ioi_gpt2_old.get_test_accuracy(model), ioi_gpt2.get_test_loss(model).item(), ioi_gpt2.get_test_accuracy(model), sports_gpt2.get_test_loss(model).item(), sports_gpt2.get_test_accuracy(model), owt_gpt2.get_test_loss(model).item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IOI-old Loss</th>\n",
       "      <th>IOI-old Accuracy</th>\n",
       "      <th>IOI Loss</th>\n",
       "      <th>IOI Accuracy</th>\n",
       "      <th>Sports Loss</th>\n",
       "      <th>Sports Accuracy</th>\n",
       "      <th>OWT Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pythia-70m</th>\n",
       "      <td>6.674894</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.765071</td>\n",
       "      <td>0.47</td>\n",
       "      <td>4.327347</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>3.774127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-160m</th>\n",
       "      <td>1.510956</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.605831</td>\n",
       "      <td>0.93</td>\n",
       "      <td>5.229007</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3.382833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-410m</th>\n",
       "      <td>1.145940</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.312516</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.831539</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>2.731288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-1b</th>\n",
       "      <td>1.439045</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.304217</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.348832</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>2.732046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-1.4b</th>\n",
       "      <td>1.146600</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.305690</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.003749</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.546326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythia-2.8b</th>\n",
       "      <td>1.659012</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.607781</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.150230</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>2.584105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-small</th>\n",
       "      <td>0.446071</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.531604</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.938488</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>4.077174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-medium</th>\n",
       "      <td>0.733108</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.827779</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.577617</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>3.743425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-large</th>\n",
       "      <td>0.778143</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.801914</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.386537</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>3.416237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2-xl</th>\n",
       "      <td>1.086272</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.010668</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.568901</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>3.477935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             IOI-old Loss  IOI-old Accuracy  IOI Loss  IOI Accuracy  \\\n",
       "pythia-70m       6.674894              0.50  6.765071          0.47   \n",
       "pythia-160m      1.510956              0.98  1.605831          0.93   \n",
       "pythia-410m      1.145940              1.00  1.312516          1.00   \n",
       "pythia-1b        1.439045              0.99  1.304217          1.00   \n",
       "pythia-1.4b      1.146600              1.00  1.305690          1.00   \n",
       "pythia-2.8b      1.659012              1.00  1.607781          1.00   \n",
       "gpt2-small       0.446071              1.00  0.531604          1.00   \n",
       "gpt2-medium      0.733108              1.00  0.827779          1.00   \n",
       "gpt2-large       0.778143              1.00  0.801914          1.00   \n",
       "gpt2-xl          1.086272              1.00  1.010668          1.00   \n",
       "\n",
       "             Sports Loss  Sports Accuracy  OWT Loss  \n",
       "pythia-70m      4.327347         0.360000  3.774127  \n",
       "pythia-160m     5.229007         0.428571  3.382833  \n",
       "pythia-410m     1.831539         0.530000  2.731288  \n",
       "pythia-1b       1.348832         0.785714  2.732046  \n",
       "pythia-1.4b     1.003749         0.900000  2.546326  \n",
       "pythia-2.8b     0.150230         0.928571  2.584105  \n",
       "gpt2-small      2.938488         0.357143  4.077174  \n",
       "gpt2-medium     1.577617         0.830000  3.743425  \n",
       "gpt2-large      1.386537         0.642857  3.416237  \n",
       "gpt2-xl         0.568901         0.860000  3.477935  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    750\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/data/phillip_guo/mechanistic-unlearning/tasks/task_testing.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/mechanistic-unlearning/tasks/task_testing.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pythia_tokenizer([\u001b[39m'\u001b[39;49m\u001b[39m Erica\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Samuel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Joshua\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Nicole\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Rebecca\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Lindsey\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Nicole\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m Gregory\u001b[39;49m\u001b[39m'\u001b[39;49m], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2799\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2885\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2886\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2887\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2888\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2889\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2890\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2891\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2892\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2893\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2894\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2895\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2896\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2897\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2898\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2899\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2900\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2902\u001b[0m     )\n\u001b[1;32m   2903\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:552\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:764\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    760\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    761\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    762\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    763\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    765\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "pythia_tokenizer([' Erica', ' Samuel', ' Joshua', ' Nicole', ' Rebecca', ' Lindsey', ' Nicole', ' Gregory'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tasks.inference_utils import generate_text\n",
    "# generate_text(models[0], pythia_tokenizer, , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
